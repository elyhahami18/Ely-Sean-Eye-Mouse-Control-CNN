{"cells":[{"cell_type":"code","execution_count":null,"id":"27c87e2a","metadata":{"id":"27c87e2a","outputId":"8e2ccf58-5dfb-4f07-8381-59c7057bdde4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/elyhahami/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.0\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"id":"c6122b9c","metadata":{"id":"c6122b9c","outputId":"6d7e52f4-52ed-491b-cd37-186dd5258a4d"},"outputs":[{"data":{"text/plain":["['0001',\n"," '0002',\n"," '0003',\n"," '0004',\n"," '0005',\n"," '0006',\n"," '0007',\n"," '0008',\n"," '0009',\n"," '0010',\n"," '0011',\n"," '0012',\n"," '0013',\n"," '0014',\n"," '0015',\n"," '0016',\n"," '0017',\n"," '0018',\n"," '0019',\n"," '0020',\n"," '0021',\n"," '0022',\n"," '0023',\n"," '0024',\n"," '0025',\n"," '0026',\n"," '0027',\n"," '0028',\n"," '0029',\n"," '0030',\n"," '0031',\n"," '0032',\n"," '0033',\n"," '0034',\n"," '0035',\n"," '0036',\n"," '0037',\n"," '0038',\n"," '0039',\n"," '0040',\n"," '0041',\n"," '0042',\n"," '0043',\n"," '0044',\n"," '0045',\n"," '0046',\n"," '0047',\n"," '0048',\n"," '0049',\n"," '0050',\n"," '0051',\n"," '0052',\n"," '0053',\n"," '0054',\n"," '0055',\n"," '0056']"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import cv2\n","\n","dataset_path = \"/Users/elyhahami/Downloads/Columbia Gaze Data Set\"\n","file_list = sorted(os.listdir(dataset_path))\n","file_list.pop(0)\n","file_list"]},{"cell_type":"code","execution_count":null,"id":"21a1fe94","metadata":{"id":"21a1fe94","outputId":"03cc12d6-1c2e-4734-80a8-2e4a1bf13cd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["[-15 -15 -15 ...  30  30  30]\n","5880\n","5880\n"]}],"source":["#can optimize this code by: 1) improve variable names; use numpy to deal with flattening arrays\n","##also need to get training labels\n","#can also do data augmentation\n","import numpy as np\n","import os\n","import cv2\n","my_list = []\n","dataset_path = \"/Users/elyhahami/Downloads/Columbia Gaze Data Set\"\n","\n","# Get a list of subject folders in the dataset\n","file_list = sorted(os.listdir(dataset_path))\n","file_list.pop(0)\n","file_list\n","# Iterate over each subject folder\n","for subject_folder in file_list:\n","    subject_folder_path = os.path.join(dataset_path, subject_folder)\n","\n","    # Get a list of photo files in the subject folder\n","    photo_files = sorted(os.listdir(subject_folder_path))\n","    my_list.append(photo_files)\n","\n","# print(my_list)\n","labels_list = []\n","empty_list = []\n","bad_list = ['Thumbs.db','.picasa.ini', '.DS_Store']\n","for i in range(len(my_list)):\n","    photo_name_list = my_list[i]\n","    new_list = [item for item in photo_name_list if item not in bad_list]\n","    for j in new_list:\n","        label = j.split('_')[2][:-1]\n","        labels_list.append(label)\n","    empty_list.append(new_list)\n","#finish something like this\n","# print(empty_list)\n","\n","one_dim_empty_list = [element for sublist in empty_list for element in sublist]\n","# print(len(one_dim_empty_list))\n","# # print(one_dim_empty_list)\n","# print(labels_list)\n","# print(len(labels_list))\n","models_train_labels = np.array([int(x) for x in labels_list])\n","print(models_train_labels)\n","print(len(models_train_labels))\n","print(len(one_dim_empty_list))"]},{"cell_type":"code","execution_count":null,"id":"373f7bbc","metadata":{"id":"373f7bbc"},"outputs":[],"source":["def process_images(starting, ending):\n","    processed_images = []\n","\n","    for image_path in one_dim_empty_list[starting:ending]:\n","        full_image_path = dataset_path + '/' + image_path\n","        lolol = full_image_path.split('/')\n","        number_path = lolol[5]\n","        important_num = number_path[0:4]\n","\n","        newest_full_image_path = dataset_path + '/' + str(important_num) + '/' + image_path\n","        image = cv2.imread(newest_full_image_path)\n","        resized_image = cv2.resize(image, (224, 224))# Resize the image to the desired dimensions (prop to div by 10)\n","        normalized_image = resized_image / 255.0 # Normalize the pixel values between 0 and 1\n","\n","        processed_images.append(normalized_image)\n","\n","\n","    return processed_images"]},{"cell_type":"code","execution_count":null,"id":"907a55b2","metadata":{"id":"907a55b2"},"outputs":[],"source":["def concatenate_all_images():\n","    batch_size = 100  # Adjust batch size according to your requirements\n","    starting_index = 0\n","    all_images = []\n","\n","    while starting_index < len(one_dim_empty_list):\n","        processed_batch = process_images(starting_index, starting_index + batch_size)\n","        all_images.extend(processed_batch)\n","        starting_index += batch_size\n","\n","    return all_images"]},{"cell_type":"code","execution_count":null,"id":"44ae99a3","metadata":{"id":"44ae99a3"},"outputs":[],"source":["total_train_images = concatenate_all_images()"]},{"cell_type":"code","execution_count":null,"id":"12b2b43b","metadata":{"id":"12b2b43b"},"outputs":[],"source":["#writing test case to see correspondence between models_train_labels and total_train_images:"]},{"cell_type":"code","execution_count":null,"id":"0b74f323","metadata":{"id":"0b74f323"},"outputs":[],"source":["##one-hot encoding\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","label_encoder = LabelEncoder()\n","integer_labels = label_encoder.fit_transform(models_train_labels)\n","one_hot_labels = to_categorical(integer_labels, num_classes=5)"]},{"cell_type":"code","execution_count":null,"id":"55acb72f","metadata":{"id":"55acb72f"},"outputs":[],"source":["total_train_images = np.array(total_train_images)"]},{"cell_type":"code","execution_count":null,"id":"2729dcaf","metadata":{"id":"2729dcaf","outputId":"5c49bb0e-0cf2-46c8-dddc-a315c9b0f836"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," global_average_pooling2d (G  (None, 512)              0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dense (Dense)               (None, 1024)              525312    \n","                                                                 \n"," batch_normalization (BatchN  (None, 1024)             4096      \n"," ormalization)                                                   \n","                                                                 \n"," dropout (Dropout)           (None, 1024)              0         \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 5125      \n","                                                                 \n","=================================================================\n","Total params: 15,249,221\n","Trainable params: 7,611,909\n","Non-trainable params: 7,637,312\n","_________________________________________________________________\n"]}],"source":["import numpy as np\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications import VGG16\n","\n","# Set the number of classes\n","num_classes = 5\n","\n","# Set the input shape for the model\n","input_shape = (224, 224, 3)  # Adjust based on your image dimensions\n","\n","# Load pre-trained VGG16 model without the top (fully connected) layers\n","base_model_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n","\n","# Add your own classification layers on top of the VGG16 model\n","x = base_model_vgg16.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)\n","x = BatchNormalization()(x)\n","x = Dropout(0.5)(x)  # Add a Dropout layer with a dropout rate of 0.5\n","\n","predictions = Dense(num_classes, activation='softmax')(x)\n","\n","# Create the VGG16-based model\n","model = Model(inputs=base_model_vgg16.input, outputs=predictions)\n","\n","# Freeze the layers of the base model\n","for layer in base_model_vgg16.layers[:-5]:\n","    layer.trainable = False\n","\n","# Compile the model\n","model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Print the model summary\n","model.summary()"]},{"cell_type":"code","execution_count":null,"id":"64ba14f0","metadata":{"id":"64ba14f0"},"outputs":[],"source":["from tensorflow.keras.callbacks import ReduceLROnPlateau\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)"]},{"cell_type":"code","execution_count":null,"id":"0b6cfd00","metadata":{"id":"0b6cfd00","outputId":"9a6b66a6-bc94-4db4-ccfa-74bd0221e16f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","37/37 [==============================] - 689s 19s/step - loss: 0.0634 - accuracy: 0.9838 - val_loss: 24.8542 - val_accuracy: 0.2764 - lr: 0.0010\n","Epoch 2/20\n","37/37 [==============================] - 672s 18s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 17.1265 - val_accuracy: 0.2534 - lr: 0.0010\n","Epoch 3/20\n","37/37 [==============================] - 667s 18s/step - loss: 8.2144e-04 - accuracy: 1.0000 - val_loss: 11.8317 - val_accuracy: 0.2500 - lr: 0.0010\n","Epoch 4/20\n","37/37 [==============================] - 666s 18s/step - loss: 3.1210e-04 - accuracy: 1.0000 - val_loss: 6.4994 - val_accuracy: 0.4065 - lr: 0.0010\n","Epoch 5/20\n","37/37 [==============================] - 666s 18s/step - loss: 2.1932e-04 - accuracy: 1.0000 - val_loss: 2.9500 - val_accuracy: 0.5553 - lr: 0.0010\n","Epoch 6/20\n","37/37 [==============================] - 667s 18s/step - loss: 1.6141e-04 - accuracy: 1.0000 - val_loss: 1.7073 - val_accuracy: 0.6794 - lr: 0.0010\n","Epoch 7/20\n","37/37 [==============================] - 670s 18s/step - loss: 1.1062e-04 - accuracy: 1.0000 - val_loss: 0.9190 - val_accuracy: 0.8257 - lr: 0.0010\n","Epoch 8/20\n","37/37 [==============================] - 668s 18s/step - loss: 8.6847e-05 - accuracy: 1.0000 - val_loss: 0.5176 - val_accuracy: 0.9133 - lr: 0.0010\n","Epoch 9/20\n","37/37 [==============================] - 669s 18s/step - loss: 7.1007e-05 - accuracy: 1.0000 - val_loss: 0.5496 - val_accuracy: 0.8920 - lr: 0.0010\n","Epoch 10/20\n","37/37 [==============================] - 666s 18s/step - loss: 7.5353e-05 - accuracy: 1.0000 - val_loss: 0.4930 - val_accuracy: 0.8954 - lr: 0.0010\n","Epoch 11/20\n","37/37 [==============================] - 664s 18s/step - loss: 5.2071e-05 - accuracy: 1.0000 - val_loss: 0.3256 - val_accuracy: 0.9303 - lr: 0.0010\n","Epoch 12/20\n","37/37 [==============================] - 663s 18s/step - loss: 3.9889e-05 - accuracy: 1.0000 - val_loss: 0.8012 - val_accuracy: 0.8546 - lr: 0.0010\n","Epoch 13/20\n","37/37 [==============================] - 664s 18s/step - loss: 3.0219e-05 - accuracy: 1.0000 - val_loss: 0.8518 - val_accuracy: 0.8503 - lr: 0.0010\n","Epoch 14/20\n","37/37 [==============================] - ETA: 0s - loss: 2.0032e-05 - accuracy: 1.0000 \n","Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","37/37 [==============================] - 665s 18s/step - loss: 2.0032e-05 - accuracy: 1.0000 - val_loss: 1.2508 - val_accuracy: 0.8104 - lr: 0.0010\n","Epoch 15/20\n","37/37 [==============================] - 667s 18s/step - loss: 1.6988e-05 - accuracy: 1.0000 - val_loss: 0.8046 - val_accuracy: 0.8529 - lr: 5.0000e-04\n","Epoch 16/20\n","37/37 [==============================] - 665s 18s/step - loss: 1.5666e-05 - accuracy: 1.0000 - val_loss: 0.5400 - val_accuracy: 0.8759 - lr: 5.0000e-04\n","Epoch 17/20\n","37/37 [==============================] - ETA: 0s - loss: 1.4791e-05 - accuracy: 1.0000 \n","Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","37/37 [==============================] - 664s 18s/step - loss: 1.4791e-05 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8801 - lr: 5.0000e-04\n","Epoch 18/20\n","37/37 [==============================] - 665s 18s/step - loss: 1.3116e-05 - accuracy: 1.0000 - val_loss: 0.3291 - val_accuracy: 0.9184 - lr: 2.5000e-04\n","Epoch 19/20\n","37/37 [==============================] - 665s 18s/step - loss: 1.4462e-05 - accuracy: 1.0000 - val_loss: 0.2922 - val_accuracy: 0.9133 - lr: 2.5000e-04\n","Epoch 20/20\n","37/37 [==============================] - 665s 18s/step - loss: 1.4210e-05 - accuracy: 1.0000 - val_loss: 0.2752 - val_accuracy: 0.9065 - lr: 2.5000e-04\n"]}],"source":["history = model.fit(x=total_train_images, y=one_hot_labels,\n","          validation_split=0.2, batch_size = 128, epochs = 20, shuffle = True, verbose = 1,\n","                   callbacks=[reduce_lr]) # callbacks=[stop_early]"]},{"cell_type":"code","execution_count":null,"id":"3afdb59e","metadata":{"id":"3afdb59e"},"outputs":[],"source":["model.save(\"most_updated_head_position_prediction.h5\")"]},{"cell_type":"code","execution_count":null,"id":"9dc4e751","metadata":{"id":"9dc4e751","outputId":"22ed3b28-4cc4-48e4-9e9b-30a820c1354d"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: pre-trained-VGG16-gaze-detection/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: pre-trained-VGG16-gaze-detection/assets\n"]}],"source":["# model.save(\"pre-trained-VGG16-gaze-detection\")"]},{"cell_type":"code","execution_count":null,"id":"484d68db","metadata":{"id":"484d68db"},"outputs":[],"source":["# def head_direction_predictions(path):\n","#     import cv2\n","#     import numpy as np\n","\n","#     # Load and preprocess the image\n","#     image_path = path\n","#     image = cv2.imread(image_path)\n","#     resized_image = cv2.resize(image, (224, 224))  # Resize the image to match the input size of your model\n","#     normalized_image = resized_image / 255.0  # Normalize the pixel values between 0 and 1\n","#     processed_image = np.expand_dims(normalized_image, axis=0)  # Add an extra dimension for batch size\n","#     predictions = model.predict(processed_image)\n","#     class_labels = [\"-30 degrees\", \"-15 degrees\", \"0 degrees\", \"15 degrees\", \"30 degrees\"]\n","#     predicted_class_index = np.argmax(predictions)\n","#     predicted_class_label = class_labels[predicted_class_index]\n","#     return \"Predicted head position:\", predicted_class_label"]},{"cell_type":"code","execution_count":null,"id":"9139412e","metadata":{"id":"9139412e","outputId":"f36da0a9-d085-4201-acae-7bb0fbc957ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 136ms/step\n"]},{"data":{"text/plain":["('Predicted head position:', '0 degrees')"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# head_direction_predictions(\"/Users/elyhahami/Downloads/Columbia Gaze Data Set/0003/0003_2m_0P_0V_-5H.jpg\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}